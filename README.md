# ğŸ“š RAG Search with Evaluation Dashboard

This tool allows you to upload PDF documents, ask questions about them, and receive answers generated using Retrieval-Augmented Generation (RAG). It also evaluates the quality of each response using faithfulness and relevance metrics.

---

## ğŸš€ Features

- ğŸ“¥ Upload multiple PDF files
- ğŸ§  Chunk and embed the documents
- ğŸ” Ask natural language questions
- ğŸ¤– Receive answers powered by LLMs + document context
- ğŸ“ˆ Visualize monthly evaluation metrics
- ğŸ“Š Faithfulness & Relevance score charts
- ğŸ” Paste your own OpenAI API key securely
- ğŸ“¬ Logs stored for later analysis

---

## ğŸ§© Tech Stack

- **Frontend/UI**: Streamlit
- **Backend AI**: OpenAI API (configurable key)
- **Embeddings**: FAISS Vector Store + LangChain
- **Evaluation**: Custom evaluator logic
- **Charts**: Altair
- **PDF Parsing**: LangChain-compatible loaders

---

## ğŸ›  Installation

```bash
git clone https://github.com/yourusername/rag-eval-dashboard.git
cd rag-eval-dashboard
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
```

---

## âš™ï¸ Usage

```bash
streamlit run app.py
```

Then open your browser to: [http://localhost:8501](http://localhost:8501)

---

## ğŸ” API Key Required

To use the app, youâ€™ll need an OpenAI API key. You can get it here:  
ğŸ‘‰ https://platform.openai.com/account/api-keys

Paste the key into the input field at the top of the app.

---

## ğŸ“¦ Folder Structure

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ chunking.py
â”œâ”€â”€ config.py
â”œâ”€â”€ embedding_utils.py
â”œâ”€â”€ evaluator.py
â”œâ”€â”€ rag_chain.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ parser.py
â”œâ”€â”€ data/uploads/
â”œâ”€â”€ vectorstore/
â””â”€â”€ eval_results/
```
